# Компьютерная система учебной аналитики для поддержки качества тестовых баз Moodle

**Черновик статьи для секции «Компьютерные системы и информационные технологии» журнала «Вестник ИТМО»**  
ISSN 2226-1494 (PRINT), ISSN 2500-0373 (ONLINE)

---

## Введение

Встроенные инструменты учебной аналитики системы управления обучением Moodle позволяют преподавателю получить базовую статистику по тестам: индексы легкости и дискриминации, распределение ответов, попытки прохождения. Однако для целенаправленного улучшения тестовой базы этих данных часто недостаточно. Преподавателю приходится вручную сопоставлять отчёты из разных разделов Moodle, выгружать данные в сторонние инструменты или анализировать крупные HTML- и CSV-файлы, что отнимает значительное время и затрудняет систематическую работу над качеством банка вопросов.

В российской практике ситуацию усугубляют организационные ограничения. Не во всех вузах администрация разрешает установку дополнительных плагинов аналитики, что вынуждает использовать только стандартные средства Moodle. Кроме того, версия СДО нередко не обновляется до последних релизов, из-за чего недоступны функции авто-тегирования, современные отчёты и совместимые с новой версией расширения. В результате преподаватель часто располагает лишь стандартными выгрузками: статистика ответов по попыткам и экспорт вопросов в формате GIFT — текстовом формате Moodle для импорта и экспорта банка вопросов [1].

В данной работе представлена компьютерная система учебной аналитики, предназначенная для преподавателей, работающих с этими ограниченными источниками данных. Система выполняет сопоставление статистики ответов с экспортом банка вопросов в GIFT, автоматическую категоризацию вопросов по сложности и типу, расчёт коэффициента сбалансированности тестовой базы (KBTB) и формирование рекомендаций на основе экспертной логики. Цель разработки — снизить трудозатраты преподавателя на анализ и совершенствование тестовой базы при работе в условиях базовых возможностей Moodle.

---

## Методы

### Источники данных и архитектура системы

Система принимает на вход два типа файлов, доступных в стандартной установке Moodle без дополнительных плагинов: HTML-отчёт со статистикой ответов (экспорт результатов тестирования) и текстовый файл экспорта банка вопросов в формате GIFT. Формат GIFT поддерживает различные типы вопросов (множественный выбор, короткий ответ, соответствие и др.) и позволяет хранить тексты вопросов и вариантов ответов в одном файле [1]. Парсер извлекает из HTML-таблицы метрики: индекс легкости (доля правильных ответов), индекс дискриминации, стандартное отклонение, количество попыток и другие показатели. Из GIFT-файла извлекаются текст вопроса, тип и структура вариантов ответов.

Архитектура системы модульная: базовый слой отвечает за загрузку и парсинг данных; функциональные модули реализуют анализ вопросов, IRT-визуализацию, категоризацию и экспертную систему. Это упрощает развитие системы и независимую проверку отдельных компонентов. На рис. 1 представлен интерфейс загрузки файлов.

**Технологический стек.** Система реализована на Python 3.8+. Веб-интерфейс построен на Streamlit — фреймворке для быстрой разработки интерактивных приложений с поддержкой загрузки файлов, виджетов и отображения графиков. Для парсинга HTML-экспортов Moodle используется BeautifulSoup4; обработка табличных данных — pandas; визуализация (Person-Item Map, графики распределений) — Plotly; численные расчёты — NumPy. Текстовая схожесть при сопоставлении вопросов вычисляется с помощью SequenceMatcher из стандартной библиотеки difflib. Формат ввода/вывода GIFT обрабатывается собственным парсером на основе регулярных выражений (модуль re). Развёртывание возможно как локально, так и в облаке (например, Streamlit Cloud) без установки дополнительных серверных компонентов.

*Рис. 1. Интерфейс загрузки файлов (HTML-статистика и GIFT-экспорт)*

### Автоматическая категоризация вопросов: обзор подходов

Проблема автоматической категоризации тестовых вопросов активно исследуется в контексте образовательной аналитики и адаптивного тестирования.

В работах по автоматической генерации и классификации заданий [2, 3] рассматриваются методы масштабируемого создания и категоризации вопросов. Исследования показывают, что ручное создание и классификация заданий трудоёмки; для поддержки цифрового оценивания разрабатываются подходы на основе машинного обучения и обработки естественного языка.

Классификация вопросов по уровням когнитивной сложности (таксономия Блума) реализована в ряде систем с использованием комбинаций классификаторов (голосование, ансамбли) для автоматического отнесения вопросов к уровням [4]. Аналогичные методы применяются для категоризации по трудности (лёгкий, средний, сложный) с использованием Random Forest, логистической регрессии и нейросетевых моделей, достигая точности порядка 84% при предсказании сложности по тексту [5, 6].

В то же время многие исследования опираются на предобученные модели, крупные размеченные корпуса или доступ к API современных СДО. В условиях ограниченных выгрузок (CSV/HTML + GIFT) и отсутствия разметки предпочтительны правила на основе статистических метрик и текстовой схожести.

В данной системе категоризация выполняется по следующим правилам. Вопросы сопоставляются между статистикой (HTML) и банком (GIFT) по нормализованному тексту с использованием меры схожести SequenceMatcher (коэффициент от 0 до 1). При пороге 0,9 вопросы считаются совпадающими. Для несопоставленных вопросов используются данные только из GIFT со значениями по умолчанию. Далее вопросы распределяются по категориям:

1. **По сложности (индекс легкости):** лёгкие (≥70%), средние и сложные (&lt;70%).
2. **По типу ответа:** открытые (короткий ответ, эссе, соответствие) и закрытые (множественный выбор, верно/неверно).
3. **На переделку:** низкая дискриминация (&lt;0,3), входящие в верхние 10% по лёгкости (самые «простые»), либо мало попыток (ниже порога надёжности по правилу Наннали или 25-му перцентилю). Граница «лёгкие» (≥X%) настраивается в диапазоне 65–75%.

Дополнительно выполняется поиск дубликатов: вопросы с идентичными метриками (тип, текст, сложность, дискриминация и др.) объединяются в один представитель с пометкой позиций (например, «5.1 (6.1)»). При расчёте KBTB и других метрик учитывается фактическое количество уникальных вопросов без повторений.

### Коэффициент сбалансированности тестовой базы (KBTB)

Для интегральной оценки качества тестовой базы введён коэффициент KBTB (коэффициент сбалансированности тестовой базы). Он оценивает соответствие фактического состава банка целевым пропорциям по типам вопросов, уровням сложности и доле заданий, требующих переработки.

Пусть заданы целевые доли: по типу — открытые (O) и закрытые (Z) с суммой 1; по уровню сложности — лёгкие (L), средние (M) и трудные (H) с суммой 1. Фактические доли обозначим как a_O, a_Z, a_L, a_M, a_H. Отклонения:

$$D_{\text{тип}} = 0{,}5 \cdot (|a_O - t_O| + |a_Z - t_Z|)$$

$$D_{\text{уровень}} = 0{,}5 \cdot (|a_L - t_L| + |a_M - t_M| + |a_H - t_H|)$$

где t_O, t_Z, t_L, t_M, t_H — целевые доли. Обе величины лежат в отрезке [0, 1].

Доля вопросов «на переделку» R определяется как доля заданий с дискриминацией &lt;0,3, входящих в верхние 10% по лёгкости либо с малым числом попыток. Штраф за переделку моделируется как:

$$P_{\text{переделка}} = 1 - e^{-3R}$$

Дополнительно можно задать минимальное число вопросов n_мин; при n &lt; n_мин вводится штраф P_количество = 1 − n/n_мин, иначе P_количество = 0.

Коэффициент KBTB вычисляется по формуле:

$$\text{KBTB} = 1 - w_1 D_{\text{тип}} - w_2 D_{\text{уровень}} - w_3 P_{\text{переделка}} - w_4 P_{\text{количество}}$$

с весами w_1 = 0,3, w_2 = 0,3, w_3 = 0,2, w_4 = 0,2. Значение KBTB ограничивается отрезком [0, 1]. Интерпретация: ≥0,85 — отлично сбалансировано; ≥0,70 — хорошо; ≥0,50 — есть перекосы; &lt;0,50 — требуется переработка. Преподаватель может настраивать целевые доли под специфику дисциплины и желаемый баланс типов и сложности.

В интерфейсе системы доли сложности задаются связанными слайдерами: пользователь регулирует долю лёгких (L) и средних (M) вопросов; доля сложных (H) вычисляется автоматически как H = 100 − L − M. При увеличении L верхняя граница слайдера M уменьшается; при изменении M значение H пересчитывается. Такая схема обеспечивает соблюдение условия L + M + H = 100% и исключает некорректные комбинации.

### Пометка вопросов на доработку в Moodle: встроенные средства и ограничения

В стандартной установке Moodle анализ качества вопросов представлен отчётом «Статистика теста» (Quiz statistics report) [7, 8]. Отчёт вычисляет индекс легкости (Facility Index) — долю правильных ответов в процентах; индекс дискриминации (Discrimination Index) — корреляцию балла по вопросу с общим баллом теста; эффективность дискриминации (Discriminative Efficiency) — долю максимально достижимой дискриминации при данной лёгкости вопроса [8]. При очень низкой дискриминации ячейки в таблице подсвечиваются красным как потенциально проблемные [9], однако никакой автоматической пометки «требует доработки» в банке вопросов Moodle не предусмотрено.

Флаг «отметить вопрос» (Flag) в Moodle относится к попытке студента: студент помечает вопрос во время прохождения теста для последующей проверки. Преподаватель видит состояние флага, но не может изменить его; этот механизм не связан с качеством вопроса и не интегрирован с банком [10]. В банке вопросов доступны комментарии, статус черновика и теги [11], но критерии «нужна доработка» не формализованы — преподаватель определяет их сам, просматривая статистику вручную.

Ограничения встроенной аналитики Moodle: (1) статистика привязана к конкретному тесту и попыткам, а не к банку в целом; (2) при использовании случайных вопросов агрегированные показатели по «позиции» бессмысленны, требуется разбор по отдельным вопросам; (3) нет интегральной метрики сбалансированности банка; (4) нет категоризации вопросов по типу и сложности; (5) экспорт в HTML/CSV не содержит структурированных пометок о качестве. Таким образом, для целенаправленной работы над банком вопросов преподавателю приходится вручную сопоставлять отчёт со статистикой и экспорт банка в GIFT, что мотивирует разработку внешней системы аналитики.

### Экспертная система и IRT-визуализация

Экспертные системы и системы рекомендаций в образовательной аналитике реализуют два основных подхода: на основе правил (rule-based) и на основе машинного обучения. Правила обеспечивают прозрачность и интерпретируемость выводов; в ряде исследований rule-based методы по выявлению дефектов формулировок вопросов превосходили большие языковые модели по точности [12]. Экспертные системы на основе правил применяются для автоматической классификации вопросов по компетенции и уровню сложности при валидации преподавателями [13]. Системы образовательных рекомендаций могут использовать «траекторию эксперта» для формирования советов по освоению материала и целевых интервенций при выявлении студентов группы риска [14, 15].

Модуль экспертной системы в разработанной системе построен на правилах и эвристиках. Входные данные — список вопросов с метриками (сложность, дискриминация, тип) и, при наличии, оценки способностей студентов. Система выполняет: (1) анализ распределения способностей студентов (низкие/средние/высокие по порогам μ ± σ); (2) анализ распределения сложности вопросов (легкие/средние/сложные, баланс); (3) выявление вопросов с низкой дискриминацией (&lt;0,3); (4) оценку перекрытия распределений способностей и сложности в логит-шкале; (5) формирование рекомендаций на естественном языке.

Правила рекомендаций: при доле студентов с низкими способностями &gt;40% — предложение дополнительной поддержки; при доле сильных &gt;40% — предложение усложнить задания; при смещении к легким или сложным вопросам — коррекция баланса; при низком перекрытии распределений — добавление вопросов средней сложности; при выявлении вопросов с малым числом попыток — рекомендация протестировать больше студентов с указанием номеров вопросов. Рекомендации по вопросам с низкой дискриминацией и малым числом попыток выводятся с номерами (с учётом дубликатов, напр. 5.1 (6.1)). При рекомендациях вида «слишком много лёгких — добавьте сложные» или «слишком много сложных — добавьте лёгкие» система выдаёт ориентировочное количество вопросов на основе целевых долей L/M/H, настроенных в блоке KBTB: например, «(ориентировочно +N сложных)», где N = max(0, ⌊n·t_H⌋ − n_H), n — число вопросов в банке, t_H — целевая доля сложных, n_H — фактическое число сложных вопросов. Рекомендации дополняют значение коэффициента KBTB.

Person-Item Map реализован как визуализация в логит-шкале: сложность вопросов преобразуется из доли правильных ответов (индекс легкости) в логит, распределение способностей студентов при отсутствии индивидуальных данных моделируется нормальным на основе среднего и стандартного отклонения сложности. График показывает соответствие банка вопросов ожидаемому уровню подготовки и выявляет «пустые» зоны шкалы (рис. 4). Это даёт наглядное представление о согласованности банка, хотя при отсутствии реальных данных о способностях студентов полная IRT-калибровка недоступна.

### IRT и коэффициент перекрытия в СДО: контекст и связь с KBTB

Нативная поддержка IRT в массово распространённых СДО (Moodle, Canvas, Blackboard, D2L) отсутствует: эти платформы опираются на классическую тестовую теорию — индекс легкости, дискриминации, коэффициент внутренней согласованности (CIC, эквивалентен α Кронбаха) [7, 8]. IRT-анализ интегрируют через расширения или внешние инструменты: данные выгружают из СДО и обрабатывают в R, Python (пакеты eRm, mirt, TAM и др.) или специализированном ПО [16, 17]. Person-Item Map и родственные визуализации используются в психометрике для оценки соответствия трудности заданий и способностей испытуемых; при отсутствии IRT-калибровки применяют приближения на основе доли правильных ответов и логит-преобразования [18].

Коэффициент перекрытия в данной системе — доля пересечения диапазонов распределений способностей и сложности вопросов в логит-шкале: K_перекрытия = (overlap_range / total_range) × 100%. Низкое перекрытие (&lt;30%) указывает на рассогласование: либо вопросы слишком легки/трудны для выборки, либо есть «пустые» участки шкалы. KBTB и коэффициент перекрытия дополняют друг друга: KBTB оценивает сбалансированность банка по типам, уровням сложности и доле заданий на переделку, не требуя данных о студентах; коэффициент перекрытия — соответствие банка конкретной выборке испытуемых. Высокий KBTB при низком перекрытии означает, что банк структурно сбалансирован, но не подходит данной когорте (нужна коррекция сложности или контингента). Низкий KBTB при высоком перекрытии — банк плохо структурирован, хотя локально подходит выборке. Целевая ситуация — высокие оба показателя.

### Валидация улучшения банка: метрики и эксперимент

После категоризации и переработки вопросов необходимо оценить, что банк действительно улучшился. Метрики надёжности и согласованности теста [8, 19]:

- **Коэффициент внутренней согласованности (α Кронбаха, CIC)** — наиболее распространённая метрика; Moodle вычисляет её в отчёте «Статистика теста». Интерпретация: CIC &gt;0,75 — удовлетворительно; &lt;0,64 — неудовлетворительно. Рост α после переработки банка свидетельствует о повышении согласованности заданий.

- **λ₆ Гуттмана** — альтернатива α при многомерности или непараллельности заданий; может давать иные оценки при тех же данных [19].

- **Надёжность расщепления (split-half)** — тест делят на две половины (чётные/нечётные номера или случайное разбиение), вычисляют корреляцию Пирсона r между половинами, затем применяют формулу Спирмена–Брауна: α_pred = 2r / (1 + r). Метод чувствителен к эквивалентности половин; при сильном различии дисперсий предпочтительна формула Фланагана–Рулона [20].

- **Средняя межпунктовая корреляция** — дополнительный индикатор однородности; слишком высокие значения могут указывать на семантическое перекрытие заданий и завышение α [21].

Шаблон описания эксперимента по валидации улучшения банка приведён в приложении.

---

## Основные результаты

Система реализована как веб-приложение с пошаговой работой: загрузка HTML-статистики и GIFT-файла, сопоставление вопросов, категоризация и расчёт KBTB. Пользователь получает:

1. Таблицу сопоставленных вопросов с метриками и предупреждениями о дубликатах и несопоставленных записях.
2. Разбиение вопросов на категории: лёгкие открытые, лёгкие закрытые, средние+сложные открытые, средние+сложные закрытые, на переделку. Пользователь может варьировать границу «лёгкие» (слайдер 65–75%) перед экспортом. Категории при экспорте в GIFT имеют плоскую структуру с суффиксом (Lappka) (рис. 2).
3. Значение KBTB с разбивкой вклада каждого штрафа — тип, уровень, переделка, количество (рис. 3).
4. Person-Item Map и текстовые рекомендации экспертной системы (рис. 4).

Экспорт категоризированного банка обратно в GIFT позволяет преподавателю обновлять банк вопросов в Moodle с учётом проведённого анализа.

*Рис. 2. Результаты категоризации вопросов по типам и сложности*

*Рис. 3. Блок расчёта KBTB с целевыми долями и интерпретацией*

*Рис. 4. Person-Item Map: распределение сложности вопросов и способностей студентов*

---

## Обсуждение и заключение

Разработанная система предназначена для сценария, в котором преподаватель ограничен базовыми выгрузками Moodle и не может устанавливать расширенные плагины. Использование стандартных форматов (HTML-отчёт и GIFT) обеспечивает совместимость со старыми и новыми версиями СДО.

Коэффициент KBTB даёт компактную интегральную метрику сбалансированности банка и позволяет отслеживать её изменение при добавлении или изменении вопросов. Правила категоризации, основанные на статистических метриках и текстовой схожести, не требуют обучения моделей и работают при малом объёме данных.

Ограничением подхода является зависимость от качества экспорта: расхождения в кодировке, форматировании или структуре HTML/GIFT могут приводить к ошибкам сопоставления. Категоризация по типологии «открытый/закрытый» и трём уровням сложности является упрощённой по сравнению с таксономией Блума или более тонкими IRT-моделями. Для повышения точности в будущем возможно привлечение методов машинного обучения при наличии размеченных данных.

В перспективе планируется валидация KBTB на реальных тестовых базах вузов-партнёров и сопоставление с экспертными оценками преподавателей.

---

## Список литературы

1. GIFT format // MoodleDocs. URL: https://docs.moodle.org/en/GIFT_format (дата обращения: 25.01.2025).

2. Gierl M. J., Lai H. Automatic item generation: foundations and machine learning-based approaches for assessments // Frontiers in Education. 2023. Vol. 8. Article 858273.

3. Kurdi G. et al. Automatic question generation and answer assessment: a survey // Research and Practice in Technology Enhanced Learning. 2021. Vol. 16. Art. 5.

4. Abduljabbar D. A., Omar N. Exam questions classification based on Bloom's taxonomy cognitive level using classifiers combination // Journal of Computational Science. 2015. Vol. 11(5). P. 741–750.

5. Roy S. et al. Text-based question difficulty prediction: a systematic review of automatic approaches // International Journal of Artificial Intelligence in Education. 2023. Vol. 34. P. 1–59.

6. Susanti Y. et al. Multiple choice question difficulty level classification with multi class confusion matrix in the online question bank of education gallery // Journal of Applied Data Sciences. 2022. Vol. 3(2). P. 92–103.

7. Quiz statistics report // MoodleDocs. URL: https://docs.moodle.org/en/Quiz_statistics_report (дата обращения: 25.01.2025).

8. Quiz statistics calculations // MoodleDocs Developer documentation. URL: https://docs.moodle.org/dev/Quiz_statistics_calculations (дата обращения: 25.01.2025).

9. Using Quiz Statistics to Identify Problematic Questions // University of Illinois. URL: https://answers.uillinois.edu/illinois.las/page.php?id=79614 (дата обращения: 25.01.2025).

10. Flagging questions during a quiz attempt // MoodleDocs Developer documentation. URL: https://docs.moodle.org/dev/Flagging_questions_during_a_quiz_attempt (дата обращения: 25.01.2025).

11. Question bank // MoodleDocs. URL: https://docs.moodle.org/en/Question_bank (дата обращения: 25.01.2025).

12. Moore S. J. Assessing the quality of multiple-choice questions using GPT-4 and rule-based methods // EC-TEL 2023. 2023.

13. Kristanto A. et al. A rule-based expert system for automatic question classification in mathematics adaptive assessment on Indonesian elementary school environment // Procedia Computer Science. 2017. Vol. 116. P. 156–163.

14. Ben Sassi I. et al. An outcome-based educational recommender system // arXiv:2509.18186. 2025.

15. Oproiu G. C. et al. An extended learning analytics framework integrating machine learning and pedagogical approaches for student performance prediction and intervention // International Journal of Artificial Intelligence in Education. 2024. URL: https://doi.org/10.1007/s40593-024-00429-7.

16. Extending LMS to Support IRT-Based Assessment Test Calibration // Advances in Web-Based Learning — ICWL 2010. Springer. 2010. P. 505–514.

17. Mair P., Hatzinger R. Extended Rasch modeling: the eRm package for the application of IRT models in R // Journal of Statistical Software. 2007. Vol. 20(9). P. 1–20.

18. Linacre J. M. Optimizing rating scale category effectiveness // Journal of Applied Measurement. 2002. Vol. 3(1). P. 85–106.

19. Tavakol M., Dennick R. Making sense of Cronbach's alpha // International Journal of Medical Education. 2011. Vol. 2. P. 53–55.

20. Khoshtarash M. et al. A comparison of the Spearman-Brown and Flanagan-Rulon formulas for split half reliability // JMASM. 2006. Vol. 5(2). P. 265–274.

21. Cortina J. M. What is coefficient alpha? An examination of theory and applications // Journal of Applied Psychology. 1993. Vol. 78(1). P. 98–104.

---

## Приложение. Шаблон описания эксперимента по валидации улучшения банка вопросов

**Цель:** оценить, приводит ли категоризация и переработка вопросов на основе рекомендаций системы к статистически значимому улучшению метрик надёжности теста.

**Выборка:** [дисциплина, курс, число студентов N, число попыток тестирования до и после].

**Дизайн:** квазиэксперимент. Тест до переработки (T₁) и после (T₂) предъявлялся одной и той же или параллельной когорте студентов. Состав теста: [фиксированный / случайная выборка из банка / адаптивный].

**Метрики:**
- Коэффициент внутренней согласованности (CIC, α Кронбаха) — до и после.
- [Опционально] λ₆ Гуттмана, split-half (формула Спирмена–Брауна).
- KBTB банка до и после переработки.
- Коэффициент перекрытия (если доступны данные о способностях).

**Процедура:** (1) экспорт статистики и GIFT до переработки; (2) анализ в системе, категоризация, выявление вопросов «на переделку»; (3) переработка/замена вопросов преподавателем; (4) повторное тестирование; (5) расчёт метрик T₁ и T₂; (6) сравнение (t-тест для зависимых выборок, bootstrap-доверительные интервалы или эквивалентные методы).

**Результаты:** [таблица: метрика | до | после | Δ | p-value / 95% ДИ]. Интерпретация: рост CIC на X% (p &lt; 0,05) свидетельствует о…; KBTB вырос с A до B, что указывает на….

**Ограничения:** возможный эффект обучения при повторном тестировании; изменение контингента; малый размер выборки.

---

---

# Раздел для автора (не для публикации): основные недостатки и направления доработки

Ниже перечислены выявленные недостатки подхода и системы, которые целесообразно учесть при доработке и при рецензировании статьи.

**1. Сопоставление по текстовой схожести 90%** может давать ложные совпадения при схожих формулировках разных вопросов или, напротив, пропускать совпадения при сильном отличии разметки (HTML, переносы строк) между экспортами. Стоит рассмотреть порог как настраиваемый параметр и добавить ручную верификацию критичных сопоставлений.

**2. SequenceMatcher** — простая и надёжная мера, но она чувствительна к порядку слов и не учитывает семантику. Для более точного сопоставления при наличии вычислительных ресурсов можно использовать эмбеддинги (sentence embeddings).

**3. Коэффициент KBTB — веса и пороги** заданы эвристически (0,3; 0,3; 0,2; 0,2; R «на переделку» при дискриминации &lt;0,3 и 10% самых лёгких). Они не обоснованы эмпирически на выборке вузов. Необходима валидация и, при возможности, калибровка по экспертным оценкам.

**4. Person-Item Map** при отсутствии реальных данных о способностях студентов использует сгенерированное нормальное распределение. Это даёт лишь иллюстрацию концепции, а не реальную IRT-калибровку. Для полноценного IRT-анализа требуются данные о ответах отдельных студентов (формат, отличный от агрегированной HTML-статистики).

**5. Отсутствие статистической валидации** — в статье нет количественных экспериментов (точность сопоставления, согласие с экспертами по категориям, корреляция KBTB с экспертными оценками). Это ослабляет доказательную базу и должно быть отражено в ограничениях.

**6. Дедупликация по полному совпадению метрик** объединяет вопросы с идентичной подписью (текст, тип, сложность, дискриминация и др.) в один с пометкой дублей (например, «5.1 (6.1)»). Смысловые дубликаты с немного различающимися формулировками или округлёнными метриками могут не выявляться. Возможное развитие — нечёткое сопоставление по подмножеству признаков.

**7. Экспертная система** построена на правилах и эвристиках, без документальной базы знаний и объяснимости выводов. Рекомендации полезны, но их обоснованность не формализована.

**8. Ограничение только GIFT** — Moodle поддерживает экспорт в XML и другие форматы. Расширение на Moodle XML улучшило бы совместимость и полноту данных.

**9. Русскоязычный интерфейс и GIFT** — корректность разбора зависит от кодировки (UTF-8) и соглашений Moodle. Специфика кириллицы и возможные артефакты экспорта требуют отдельной проверки на реальных выгрузках вузов.

**10. Масштабируемость** — при очень больших банках (тысячи вопросов) сопоставление «каждый с каждым» по тексту может становиться узким местом. Стоит рассмотреть индексацию или предварительную кластеризацию по типу и метрикам.

Эти пункты логично учитывать при планировании следующих версий системы и при ответах на замечания рецензентов.
